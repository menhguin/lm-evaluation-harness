{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Roadmap for next 1-2 weeks:\n",
        "* **More reliable generation/answer matching** - Basically get the Goodfire Llama 8B to near-peer with HF and VLLM on a few tasks. I think it's mainly an answer matching problem for now, since generations definitely ... they exist. I managed to get GPQA and TriviaQA generating answers, but not grading properly (?)\n",
        "* **Code cleanup and optimisation** - This was very scrappy. I wasn't even done setting up my IDE properly, so probably a few more days of just streamlining stuff.I wasn't able to set up fast batching/parallelisation, and looking at the docs there doesn't seem an obvious option for this, so I'm stuck with some slow 1-by-1 generations for now. This won't be an issue until I fix the more important bugs, but it will be annoying when I do ...\n",
        "* **Figuring out and implementing basic SAE feature methods** - I basically have to implement this while doing cleanup. Now that I'm getting any generation and any scoring going, I need to start implementing the actual fun features. But again, this is subject to designing the user end flow, so some guidance would be helpful."
      ],
      "metadata": {
        "id": "oU_AobujBxsE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsKt8d6TVnC_"
      },
      "source": [
        "#Step 1. Install EleutherAI Evaluations Harness\n",
        "*   Logging into WandB is optional.\n",
        "*   Logging into Huggingface API is required to run GPQA. This is to prevent database leakage.\n",
        "*   Uses Goodfire API! Experimental as of 11th Jan 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7G1cecrmr87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install latest versions of necessary libraries\n",
        "!pip install goodfire\n",
        "!pip install vllm\n",
        "!pip install -e git+https://github.com/menhguin/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm] # skip if you don't want to use wandb to log results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxtRDlDfA_P7"
      },
      "source": [
        "Automated login for Hugging Face Hub via Colab Secrets. If you don't have this, it'll prompt for manual login if you don't have one. If you completely remove this, you can't run GPQA or use Llama models via HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKZucZXIA97D"
      },
      "outputs": [],
      "source": [
        "# Check for Huggingface API key and log in if available, otherwise prompt for manual login\n",
        "hf_token = userdata.get('HF_READ_TOKEN')\n",
        "if hf_token:\n",
        "    huggingface_hub.login(hf_token)\n",
        "else:\n",
        "    print(\"Huggingface token not found. Please login manually.\")\n",
        "    !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpgeHaLStRb"
      },
      "source": [
        "Automated login for WandB via Colab Secrets. If you don't have this, it'll just prompt you later if you use wandb_args."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzNKgAPISUwK"
      },
      "outputs": [],
      "source": [
        "# Check for WandB API key and log in if available, otherwise skip login\n",
        "wandb_token = userdata.get('WANDB_API_KEY')\n",
        "if wandb_token:\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "else:\n",
        "    print(\"WandB token not found. Continuing without logging into WandB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated login for Goodfire API via Colab Secrets."
      ],
      "metadata": {
        "id": "HVh-4EiOA0YM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1BT5T_kWK0Q"
      },
      "outputs": [],
      "source": [
        "# Get API key from Colab secrets\n",
        "GOODFIRE_API_KEY = userdata.get('GOODFIRE_API_KEY')\n",
        "if not GOODFIRE_API_KEY:\n",
        "    raise ValueError(\"Please set GOODFIRE_API_KEY in Colab secrets\")\n",
        "os.environ['GOODFIRE_API_KEY'] = GOODFIRE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2. Run evaluation"
      ],
      "metadata": {
        "id": "2p4E-fdyBCTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gsm8k_cot_llama is currently the only one that definitely works\n",
        "sampler = \"top_p\"\n",
        "sampler_value = \"0.9\"\n",
        "tasks = \"gsm8k_cot_llama\"\n",
        "model = \"goodfire\"\n",
        "model_args = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "num_fewshot = \"8\"\n",
        "\n",
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args} \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --apply_chat_template \\\n",
        "    --fewshot_as_multiturn \\\n",
        "    --limit 30 \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},inspect=true,do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "YpP_yiYis1XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"top_p\"\n",
        "sampler_value = \"0.9\"\n",
        "tasks = \"gpqa_main_generative_n_shot\"\n",
        "model = \"goodfire\"\n",
        "model_args = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "num_fewshot = \"1\"\n",
        "\n",
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args} \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --apply_chat_template \\\n",
        "    --fewshot_as_multiturn \\\n",
        "    --limit 30 \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "j8Vh2KJWNmm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"top_p\"\n",
        "sampler_value = \"0.9\"\n",
        "tasks = \"gpqa_main_generative_n_shot\"\n",
        "model = \"goodfire\"\n",
        "model_args = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "num_fewshot = \"5\"\n",
        "\n",
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args} \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --apply_chat_template \\\n",
        "    --fewshot_as_multiturn \\\n",
        "    --limit 30 \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "Q_NDK2TemTgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"top_p\"\n",
        "sampler_value = \"0.9\"\n",
        "tasks = \"triviaqa\"\n",
        "model = \"goodfire\"\n",
        "model_args = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "num_fewshot = \"0\"\n",
        "\n",
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args} \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --apply_chat_template \\\n",
        "    --fewshot_as_multiturn \\\n",
        "    --limit 30 \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "0QkcObNb8TPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"top_p\"\n",
        "sampler_value = \"0.9\"\n",
        "tasks = \"gsm8k_cot_llama\"\n",
        "model = \"vllm\"\n",
        "model_args = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args} \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --limit 30 \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "0Bu_rc-2-nYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2PSZgKYZfCi"
      },
      "source": [
        "#Reference: EleutherAI Eval Harness task list\n",
        "For those curious to run other evals! Please note that Min P is currently only accessible for `generate_until` tasks. There is currently no easy way to index these tasks, I just Ctrl + F'd `generate_until` on the [EleutherAI Evals Harness Repo](https://github.com/EleutherAI/lm-evaluation-harness)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Goodfire Client\n",
        "import goodfire\n",
        "\n",
        "client = goodfire.Client(api_key=GOODFIRE_API_KEY)\n",
        "\n",
        "# Simple test call\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_completion_tokens=10\n",
        ")\n",
        "\n",
        "# Access response using ChatCompletion object attributes\n",
        "print(\"Test response:\", response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "xezvUGMvWPsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-RGcFC-PclJ"
      },
      "outputs": [],
      "source": [
        " !lm-eval --tasks list"
      ]
    }
  ]
}