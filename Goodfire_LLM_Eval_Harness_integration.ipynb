{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsKt8d6TVnC_"
      },
      "source": [
        "#Step 1. Install EleutherAI Evaluations Harness\n",
        "*   Logging into WandB is optional.\n",
        "*   Logging into Huggingface API is required to run GPQA. This is to prevent database leakage.\n",
        "*   Uses Goodfire API! Experimental as of Jan 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7G1cecrmr87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install latest versions of necessary libraries\n",
        "!pip install goodfire\n",
        "!pip install -e git+https://github.com/menhguin/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm] # skip if you don't want to use wandb to log results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxtRDlDfA_P7"
      },
      "source": [
        "Automated login for Hugging Face Hub via Colab Secrets. If you don't have this, it'll prompt for manual login if you don't have one. If you completely remove this, you can't run GPQA or use Llama models via HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WKZucZXIA97D"
      },
      "outputs": [],
      "source": [
        "# Check for Huggingface API key and log in if available, otherwise prompt for manual login\n",
        "hf_token = userdata.get('HF_READ_TOKEN')\n",
        "if hf_token:\n",
        "    huggingface_hub.login(hf_token)\n",
        "else:\n",
        "    print(\"Huggingface token not found. Please login manually.\")\n",
        "    !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpgeHaLStRb"
      },
      "source": [
        "Automated login for WandB via Colab Secrets. If you don't have this, it'll just prompt you later if you use wandb_args."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lzNKgAPISUwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9966aeb0-0c17-469c-8cc4-c9e8a149dc8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminh1228\u001b[0m (\u001b[33mmenhguin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "# Check for WandB API key and log in if available, otherwise skip login\n",
        "wandb_token = userdata.get('WANDB_API_KEY')\n",
        "if wandb_token:\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "else:\n",
        "    print(\"WandB token not found. Continuing without logging into WandB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated login for Goodfire API via Colab Secrets."
      ],
      "metadata": {
        "id": "HVh-4EiOA0YM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V1BT5T_kWK0Q"
      },
      "outputs": [],
      "source": [
        "# Try to get GOODFIRE_API_KEY from environment or Colab secrets\n",
        "GOODFIRE_API_KEY = os.getenv('GOODFIRE_API_KEY') or userdata.get('GOODFIRE_API_KEY')\n",
        "if not GOODFIRE_API_KEY:\n",
        "    raise ValueError(\"Please set GOODFIRE_API_KEY in environment or Colab secrets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2. Run evaluation"
      ],
      "metadata": {
        "id": "2p4E-fdyBCTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5\n",
        "import os\n",
        "\n",
        "# Set environment variable for Goodfire API key if not already set\n",
        "if 'GOODFIRE_API_KEY' not in os.environ:\n",
        "    os.environ['GOODFIRE_API_KEY'] = GOODFIRE_API_KEY  # Make sure this is defined from previous cells\n",
        "\n",
        "# Run the evaluation using the command-line interface\n",
        "!python -m lm_eval \\\n",
        "    --model goodfire \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
        "    --tasks gsm8k_cot_llama \\\n",
        "    --num_fewshot 8 \\\n",
        "    --limit 10 \\\n",
        "    --batch_size auto \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs top_p=0.9,temperature=1.0,do_sample=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyhE_FVkvav4",
        "outputId": "db928144-cfc7-4a8c-8fa8-9535d50bbfdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-11 15:46:36.174423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 15:46:36.198326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 15:46:36.205320: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 15:46:36.221929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 15:46:39.072227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-01-11:15:46:43,967 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2025-01-11:15:46:59,029 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2025-01-11:15:46:59,032 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot_llama']\n",
            "2025-01-11:15:46:59,043 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-01-11:15:46:59,043 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
            "2025-01-11:15:46:59,043 INFO     [evaluator.py:201] Initializing goodfire model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3.1-8B-Instruct'}\n",
            "README.md: 100% 7.94k/7.94k [00:00<00:00, 23.6MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.31M/2.31M [00:00<00:00, 36.5MB/s]\n",
            "test-00000-of-00001.parquet: 100% 419k/419k [00:00<00:00, 328MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 124998.64 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 188192.23 examples/s]\n",
            "2025-01-11:15:47:02,897 WARNING  [evaluator.py:270] Overwriting default num_fewshot of gsm8k_cot_llama from 8 to 8\n",
            "2025-01-11:15:47:02,899 INFO     [task.py:415] Building contexts for gsm8k_cot_llama on rank 0...\n",
            "100% 10/10 [00:00<00:00, 51.67it/s]\n",
            "2025-01-11:15:47:03,094 INFO     [evaluator.py:496] Running generate_until requests\n",
            "Running requests:   0% 0/10 [00:00<?, ?it/s]\n",
            "First prompt: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The final answer is 6\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The final answer is 5\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The final answer is 39\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The final answer is 9\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The final answer is 29\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The final answer is 33\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            "\n",
            "\n",
            "2025-01-11:15:47:09,891 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "First response: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            " Janet lays 16 eggs per day. She eats 3 for breakfast and bakes 4 with the muffins. So she has 16 - 3 - 4 = 9 eggs left. She sells them for 2 dollars each. 9 x 2 is 18. The final answer is 18\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A group of friends went to an amusement park and spent 12.50 dollars per person on food and 10.50 dollars per person on rides. If there were 5 people in the group, how much did they spend in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " For each person, they spent 12.50 + 10.50 = 23 dollars on food and rides. There were 5 people, so 23 x 5 = 115 dollars. The final answer is 115\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A store sold 100 T-shirts, some at 10 dollars each and the rest at 15 dollars each. If the amount of money they made from selling the T-shirts at 15 dollars each was 3 times the amount made from selling the T-shirts at 10 dollars each, how many T-shirts were sold at 15 dollars each?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Let x be the number of T-shirts sold at 10 dollars each. Then 100 - x T-shirts were sold at 15 dollars each. The T-shirts sold at 15 dollars each made 3 times as much money as the T-shirts sold at 10 dollars each. So 15 x (100 - x) = 3 x 10 x x. This can be simplified to 1500 - 15 x = 30 x. Adding 15 x to both sides gives 1500 = 45 x. Dividing by 45 gives 33.\n",
            "\n",
            "Running requests:  10% 1/10 [00:06<01:01,  6.80s/it]2025-01-11:15:47:11,911 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  20% 2/10 [00:08<00:31,  3.99s/it]2025-01-11:15:47:15,059 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  30% 3/10 [00:11<00:25,  3.60s/it]2025-01-11:15:47:22,005 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  40% 4/10 [00:18<00:29,  4.92s/it]2025-01-11:15:47:28,748 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  50% 5/10 [00:25<00:27,  5.58s/it]2025-01-11:15:47:35,661 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  60% 6/10 [00:32<00:24,  6.03s/it]2025-01-11:15:47:42,420 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  70% 7/10 [00:39<00:18,  6.27s/it]2025-01-11:15:47:49,352 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  80% 8/10 [00:46<00:12,  6.48s/it]2025-01-11:15:47:52,842 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  90% 9/10 [00:49<00:05,  5.55s/it]2025-01-11:15:47:59,534 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests: 100% 10/10 [00:56<00:00,  5.64s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-01-11:15:48:02,552 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2025-01-11:15:48:02,553 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot_llama\n",
            "goodfire (pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct), gen_kwargs: (top_p=0.9,temperature=1.0,do_sample=True), limit: 10.0, num_fewshot: 8, batch_size: auto\n",
            "|     Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
            "|---------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
            "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match|↑  |  0.2|±  |0.1333|\n",
            "|               |       |strict-match    |     8|exact_match|↑  |  0.2|±  |0.1333|\n",
            "\n",
            "2025-01-11 15:48:17.408291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 15:48:17.448813: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 15:48:17.462429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 15:48:17.495835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 15:48:19.317362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-01-11:15:48:23,230 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2025-01-11:15:48:38,746 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2025-01-11:15:48:38,751 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot_llama']\n",
            "2025-01-11:15:48:38,756 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-01-11:15:48:38,756 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
            "2025-01-11:15:48:38,756 INFO     [evaluator.py:201] Initializing goodfire model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3.1-8B-Instruct'}\n",
            "2025-01-11:15:48:41,779 WARNING  [evaluator.py:270] Overwriting default num_fewshot of gsm8k_cot_llama from 8 to 8\n",
            "2025-01-11:15:48:41,781 INFO     [task.py:415] Building contexts for gsm8k_cot_llama on rank 0...\n",
            "100% 10/10 [00:00<00:00, 28.47it/s]\n",
            "2025-01-11:15:48:42,135 INFO     [evaluator.py:496] Running generate_until requests\n",
            "Running requests:   0% 0/10 [00:00<?, ?it/s]\n",
            "First prompt: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The final answer is 6\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The final answer is 5\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The final answer is 39\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The final answer is 9\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The final answer is 29\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The final answer is 33\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            "\n",
            "\n",
            "2025-01-11:15:48:49,122 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "First response: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            " Janet lays 16 eggs per day. She eats 3 for breakfast and bakes 4 with the muffins. So she has 16 - 3 - 4 = 9 eggs left. She sells them for 2 dollars each. 9 x 2 is 18. The final answer is 18\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A group of friends went to an amusement park and spent 12.50 dollars per person on food and 10.50 dollars per person on rides. If there were 5 people in the group, how much did they spend in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " For each person, they spent 12.50 + 10.50 = 23 dollars on food and rides. There were 5 people, so 23 x 5 = 115 dollars. The final answer is 115\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A store sold 100 T-shirts, some at 10 dollars each and the rest at 15 dollars each. If the amount of money they made from selling the T-shirts at 15 dollars each was 3 times the amount made from selling the T-shirts at 10 dollars each, how many T-shirts were sold at 15 dollars each?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Let x be the number of T-shirts sold at 10 dollars each. Then 100 - x T-shirts were sold at 15 dollars each. The T-shirts sold at 15 dollars each made 3 times as much money as the T-shirts sold at 10 dollars each. So 15 x (100 - x) = 3 x 10 x x. This can be simplified to 1500 - 15 x = 30 x. Adding 15 x to both sides gives 1500 = 45 x. Dividing by 45 gives 33.\n",
            "\n",
            "Running requests:  10% 1/10 [00:06<01:02,  6.99s/it]2025-01-11:15:48:51,152 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  20% 2/10 [00:09<00:32,  4.07s/it]2025-01-11:15:48:54,328 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  30% 3/10 [00:12<00:25,  3.66s/it]2025-01-11:15:49:01,319 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  40% 4/10 [00:19<00:29,  4.98s/it]2025-01-11:15:49:08,074 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  50% 5/10 [00:25<00:28,  5.62s/it]2025-01-11:15:49:15,048 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  60% 6/10 [00:32<00:24,  6.08s/it]2025-01-11:15:49:21,833 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  70% 7/10 [00:39<00:18,  6.31s/it]2025-01-11:15:49:28,810 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  80% 8/10 [00:46<00:13,  6.52s/it]2025-01-11:15:49:32,322 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  90% 9/10 [00:50<00:05,  5.58s/it]2025-01-11:15:49:39,012 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests: 100% 10/10 [00:56<00:00,  5.69s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-01-11:15:49:41,731 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2025-01-11:15:49:41,733 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot_llama\n",
            "goodfire (pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct), gen_kwargs: (top_p=0.9,temperature=1.0,do_sample=True), limit: 10.0, num_fewshot: 8, batch_size: auto\n",
            "|     Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
            "|---------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
            "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match|↑  |  0.2|±  |0.1333|\n",
            "|               |       |strict-match    |     8|exact_match|↑  |  0.2|±  |0.1333|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2PSZgKYZfCi"
      },
      "source": [
        "#Reference: EleutherAI Eval Harness task list\n",
        "For those curious to run other evals! Please note that Min P is currently only accessible for `generate_until` tasks. There is currently no easy way to index these tasks, I just Ctrl + F'd `generate_until` on the [EleutherAI Evals Harness Repo](https://github.com/EleutherAI/lm-evaluation-harness)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Goodfire Client\n",
        "import goodfire\n",
        "\n",
        "client = goodfire.Client(api_key=GOODFIRE_API_KEY)\n",
        "\n",
        "# Simple test call\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_completion_tokens=10\n",
        ")\n",
        "\n",
        "# Access response using ChatCompletion object attributes\n",
        "print(\"Test response:\", response.choices[0].message['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezvUGMvWPsy",
        "outputId": "ae5e9a69-2434-4e5a-98bd-8585933c1efd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test response: Hello! It's nice to meet you. How\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-RGcFC-PclJ"
      },
      "outputs": [],
      "source": [
        " !lm-eval --tasks list"
      ]
    }
  ]
}