{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsKt8d6TVnC_"
      },
      "source": [
        "#Step 1. Install EleutherAI Evaluations Harness\n",
        "*   Logging into WandB is optional.\n",
        "*   Logging into Huggingface API is required to run GPQA. This is to prevent database leakage.\n",
        "*   Uses Goodfire API! Experimental as of Jan 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S7G1cecrmr87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: goodfire in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.4)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (0.27.2)\n",
            "Requirement already satisfied: ipywidgets<9.0.0,>=8.1.5 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (8.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (2.2.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (2.10.5)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.6.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from goodfire) (1.15.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (4.8.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (3.10)\n",
            "Requirement already satisfied: sniffio in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.2->goodfire) (0.14.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (8.31.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (4.12.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.0->goodfire) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.0->goodfire) (3.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.4.6)\n",
            "Requirement already satisfied: decorator in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.19.1)\n",
            "Requirement already satisfied: stack_data in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.6.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.3)\n",
            "Obtaining lm_eval[vllm,wandb] from git+https://github.com/menhguin/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm] (from lm_eval[vllm,wandb])\n",
            "  Updating c:\\users\\minh1\\local\\lm-evaluation-harness\\src\\lm-eval clone\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting accelerate>=0.26.0 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.16.0 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jsonlines (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting numexpr (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached numexpr-2.10.2-cp313-cp313-win_amd64.whl.metadata (8.3 kB)\n",
            "Collecting peft>=0.2.0 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb]) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval[vllm,wandb]->lm_eval[vllm,wandb])\n",
            "  Using cached sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "INFO: pip is looking at multiple versions of lm-eval to determine which version is compatible with other requirements. This could take a while.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: git+https://github.com/menhguin/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q 95ce7c50956fc0c1713b1fa7441fdf592b3ae56c\n",
            "ERROR: Could not find a version that satisfies the requirement torch>=1.8 (from lm-eval) (from versions: none)\n",
            "ERROR: No matching distribution found for torch>=1.8\n"
          ]
        }
      ],
      "source": [
        "# Remove Google Colab specific import and install commands\n",
        "import os\n",
        "import huggingface_hub\n",
        "\n",
        "# Install dependencies using pip in a separate cell if needed\n",
        "!pip install goodfire\n",
        "!pip install -e git+https://github.com/menhguin/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxtRDlDfA_P7"
      },
      "source": [
        "Automated login for Hugging Face Hub via Colab Secrets. If you don't have this, it'll prompt for manual login if you don't have one. If you completely remove this, you can't run GPQA or use Llama models via HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.3.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (75.8.0)\n",
            "Collecting wheel\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel\n",
            "Successfully installed wheel-0.45.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n",
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n",
            "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
            "    #\n",
            "    ^\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary dependencies\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip uninstall wandb -y  # Remove any existing wandb installation\n",
        "!pip install wandb --no-deps  # Install wandb without dependencies\n",
        "!pip install docker-pycreds pathtools promise protobuf psutil requests sentry-sdk setproctitle typing-extensions  # Install wandb dependencies manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WKZucZXIA97D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load .env file from specific path\n",
        "load_dotenv(\"C:/Users/minh1/local/lm-evaluation-harness/venv/.env\")\n",
        "\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    huggingface_hub.login(hf_token)\n",
        "else:\n",
        "    print(\"Huggingface token not found. Please login manually.\")\n",
        "    !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpgeHaLStRb"
      },
      "source": [
        "Automated login for WandB via Colab Secrets. If you don't have this, it'll just prompt you later if you use wandb_args."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzNKgAPISUwK",
        "outputId": "9966aeb0-0c17-469c-8cc4-c9e8a149dc8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: setuptools in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (75.8.0)\n",
            "Requirement already satisfied: wandb in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.19.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from wandb) (6.1.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from wandb) (75.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\minh1\\appdata\\roaming\\python\\python313\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\minh1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'distutils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb_token:\n\u001b[0;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m wandb_token\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     25\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\sdk\\__init__.py:25\u001b[0m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\sdk\\artifacts\\artifact.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m termerror, termlog, termwarn\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\apis\\__init__.py:44\u001b[0m\n\u001b[0;32m     41\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     46\u001b[0m reset_path()\n\u001b[0;32m     48\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInternalApi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublicApi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\apis\\public\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api, RetryingClient, requests\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     ARTIFACT_FILES_FRAGMENT,\n\u001b[0;32m      4\u001b[0m     ARTIFACTS_TYPES_FRAGMENT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     RunArtifacts,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FILE_FRAGMENT, File, Files\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\apis\\public\\api.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthread_local_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _thread_local_api_settings\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LAUNCH_DEFAULT_PROJECT\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retry, runid\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Deprecated, deprecate\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\sdk\\launch\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_launch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_and_run_agent, launch\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_launch_add\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m launch_add\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchAgent\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\sdk\\launch\\_launch.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_project_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchProject\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchAgent\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\sdk\\launch\\loader.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_docker_installed\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchError\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AbstractBuilder\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\wandb\\docker\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdockerpycreds\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_executable  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auth, www_authenticate\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\dockerpycreds\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Store\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StoreError, CredentialsNotFound\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\dockerpycreds\\store.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_environment_dict\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_executable\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mStore\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\minh1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\dockerpycreds\\utils.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspawn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
          ]
        }
      ],
      "source": [
        "# First install setuptools which includes distutils\n",
        "!pip install setuptools\n",
        "!pip install wandb\n",
        "\n",
        "# Now try the WandB login\n",
        "wandb_token = os.getenv('WANDB_API_KEY')\n",
        "\n",
        "if wandb_token:\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "else:\n",
        "    print(\"WandB token not found. Continuing without logging into WandB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVh-4EiOA0YM"
      },
      "source": [
        "Automated login for Goodfire API via Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V1BT5T_kWK0Q"
      },
      "outputs": [],
      "source": [
        "# Try to get GOODFIRE_API_KEY from environment or Colab secrets\n",
        "GOODFIRE_API_KEY = os.getenv('GOODFIRE_API_KEY') or userdata.get('GOODFIRE_API_KEY')\n",
        "if not GOODFIRE_API_KEY:\n",
        "    raise ValueError(\"Please set GOODFIRE_API_KEY in environment or Colab secrets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p4E-fdyBCTX"
      },
      "source": [
        "#Step 2. Run evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyhE_FVkvav4",
        "outputId": "db928144-cfc7-4a8c-8fa8-9535d50bbfdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-01-11 15:46:36.174423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 15:46:36.198326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 15:46:36.205320: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 15:46:36.221929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 15:46:39.072227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-01-11:15:46:43,967 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2025-01-11:15:46:59,029 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2025-01-11:15:46:59,032 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot_llama']\n",
            "2025-01-11:15:46:59,043 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-01-11:15:46:59,043 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
            "2025-01-11:15:46:59,043 INFO     [evaluator.py:201] Initializing goodfire model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3.1-8B-Instruct'}\n",
            "README.md: 100% 7.94k/7.94k [00:00<00:00, 23.6MB/s]\n",
            "train-00000-of-00001.parquet: 100% 2.31M/2.31M [00:00<00:00, 36.5MB/s]\n",
            "test-00000-of-00001.parquet: 100% 419k/419k [00:00<00:00, 328MB/s]\n",
            "Generating train split: 100% 7473/7473 [00:00<00:00, 124998.64 examples/s]\n",
            "Generating test split: 100% 1319/1319 [00:00<00:00, 188192.23 examples/s]\n",
            "2025-01-11:15:47:02,897 WARNING  [evaluator.py:270] Overwriting default num_fewshot of gsm8k_cot_llama from 8 to 8\n",
            "2025-01-11:15:47:02,899 INFO     [task.py:415] Building contexts for gsm8k_cot_llama on rank 0...\n",
            "100% 10/10 [00:00<00:00, 51.67it/s]\n",
            "2025-01-11:15:47:03,094 INFO     [evaluator.py:496] Running generate_until requests\n",
            "Running requests:   0% 0/10 [00:00<?, ?it/s]\n",
            "First prompt: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The final answer is 6\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The final answer is 5\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The final answer is 39\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The final answer is 9\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The final answer is 29\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The final answer is 33\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            "\n",
            "\n",
            "2025-01-11:15:47:09,891 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "First response: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            " Janet lays 16 eggs per day. She eats 3 for breakfast and bakes 4 with the muffins. So she has 16 - 3 - 4 = 9 eggs left. She sells them for 2 dollars each. 9 x 2 is 18. The final answer is 18\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A group of friends went to an amusement park and spent 12.50 dollars per person on food and 10.50 dollars per person on rides. If there were 5 people in the group, how much did they spend in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " For each person, they spent 12.50 + 10.50 = 23 dollars on food and rides. There were 5 people, so 23 x 5 = 115 dollars. The final answer is 115\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A store sold 100 T-shirts, some at 10 dollars each and the rest at 15 dollars each. If the amount of money they made from selling the T-shirts at 15 dollars each was 3 times the amount made from selling the T-shirts at 10 dollars each, how many T-shirts were sold at 15 dollars each?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Let x be the number of T-shirts sold at 10 dollars each. Then 100 - x T-shirts were sold at 15 dollars each. The T-shirts sold at 15 dollars each made 3 times as much money as the T-shirts sold at 10 dollars each. So 15 x (100 - x) = 3 x 10 x x. This can be simplified to 1500 - 15 x = 30 x. Adding 15 x to both sides gives 1500 = 45 x. Dividing by 45 gives 33.\n",
            "\n",
            "Running requests:  10% 1/10 [00:06<01:01,  6.80s/it]2025-01-11:15:47:11,911 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  20% 2/10 [00:08<00:31,  3.99s/it]2025-01-11:15:47:15,059 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  30% 3/10 [00:11<00:25,  3.60s/it]2025-01-11:15:47:22,005 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  40% 4/10 [00:18<00:29,  4.92s/it]2025-01-11:15:47:28,748 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  50% 5/10 [00:25<00:27,  5.58s/it]2025-01-11:15:47:35,661 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  60% 6/10 [00:32<00:24,  6.03s/it]2025-01-11:15:47:42,420 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  70% 7/10 [00:39<00:18,  6.27s/it]2025-01-11:15:47:49,352 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  80% 8/10 [00:46<00:12,  6.48s/it]2025-01-11:15:47:52,842 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  90% 9/10 [00:49<00:05,  5.55s/it]2025-01-11:15:47:59,534 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests: 100% 10/10 [00:56<00:00,  5.64s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-01-11:15:48:02,552 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2025-01-11:15:48:02,553 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot_llama\n",
            "goodfire (pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct), gen_kwargs: (top_p=0.9,temperature=1.0,do_sample=True), limit: 10.0, num_fewshot: 8, batch_size: auto\n",
            "|     Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
            "|---------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
            "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match|â†‘  |  0.2|Â±  |0.1333|\n",
            "|               |       |strict-match    |     8|exact_match|â†‘  |  0.2|Â±  |0.1333|\n",
            "\n",
            "2025-01-11 15:48:17.408291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 15:48:17.448813: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 15:48:17.462429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 15:48:17.495835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 15:48:19.317362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-01-11:15:48:23,230 INFO     [__main__.py:279] Verbosity set to INFO\n",
            "2025-01-11:15:48:38,746 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2025-01-11:15:48:38,751 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot_llama']\n",
            "2025-01-11:15:48:38,756 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-01-11:15:48:38,756 WARNING  [evaluator.py:175] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
            "2025-01-11:15:48:38,756 INFO     [evaluator.py:201] Initializing goodfire model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3.1-8B-Instruct'}\n",
            "2025-01-11:15:48:41,779 WARNING  [evaluator.py:270] Overwriting default num_fewshot of gsm8k_cot_llama from 8 to 8\n",
            "2025-01-11:15:48:41,781 INFO     [task.py:415] Building contexts for gsm8k_cot_llama on rank 0...\n",
            "100% 10/10 [00:00<00:00, 28.47it/s]\n",
            "2025-01-11:15:48:42,135 INFO     [evaluator.py:496] Running generate_until requests\n",
            "Running requests:   0% 0/10 [00:00<?, ?it/s]\n",
            "First prompt: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The final answer is 6\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The final answer is 5\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The final answer is 39\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The final answer is 9\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The final answer is 29\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The final answer is 33\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The final answer is 8\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            "\n",
            "\n",
            "2025-01-11:15:48:49,122 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "\n",
            "First response: Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
            " Janet lays 16 eggs per day. She eats 3 for breakfast and bakes 4 with the muffins. So she has 16 - 3 - 4 = 9 eggs left. She sells them for 2 dollars each. 9 x 2 is 18. The final answer is 18\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A group of friends went to an amusement park and spent 12.50 dollars per person on food and 10.50 dollars per person on rides. If there were 5 people in the group, how much did they spend in total?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " For each person, they spent 12.50 + 10.50 = 23 dollars on food and rides. There were 5 people, so 23 x 5 = 115 dollars. The final answer is 115\n",
            "\n",
            "Given the following problem, reason and give a final answer to the problem.\n",
            "Problem: A store sold 100 T-shirts, some at 10 dollars each and the rest at 15 dollars each. If the amount of money they made from selling the T-shirts at 15 dollars each was 3 times the amount made from selling the T-shirts at 10 dollars each, how many T-shirts were sold at 15 dollars each?\n",
            "Your response should end with \"The final answer is [answer]\" where [answer] is the response to the problem.\n",
            " Let x be the number of T-shirts sold at 10 dollars each. Then 100 - x T-shirts were sold at 15 dollars each. The T-shirts sold at 15 dollars each made 3 times as much money as the T-shirts sold at 10 dollars each. So 15 x (100 - x) = 3 x 10 x x. This can be simplified to 1500 - 15 x = 30 x. Adding 15 x to both sides gives 1500 = 45 x. Dividing by 45 gives 33.\n",
            "\n",
            "Running requests:  10% 1/10 [00:06<01:02,  6.99s/it]2025-01-11:15:48:51,152 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  20% 2/10 [00:09<00:32,  4.07s/it]2025-01-11:15:48:54,328 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  30% 3/10 [00:12<00:25,  3.66s/it]2025-01-11:15:49:01,319 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  40% 4/10 [00:19<00:29,  4.98s/it]2025-01-11:15:49:08,074 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  50% 5/10 [00:25<00:28,  5.62s/it]2025-01-11:15:49:15,048 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  60% 6/10 [00:32<00:24,  6.08s/it]2025-01-11:15:49:21,833 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  70% 7/10 [00:39<00:18,  6.31s/it]2025-01-11:15:49:28,810 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  80% 8/10 [00:46<00:13,  6.52s/it]2025-01-11:15:49:32,322 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests:  90% 9/10 [00:50<00:05,  5.58s/it]2025-01-11:15:49:39,012 INFO     [_client.py:1786] HTTP Request: POST https://api.goodfire.ai/api/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Running requests: 100% 10/10 [00:56<00:00,  5.69s/it]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-01-11:15:49:41,731 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
            "2025-01-11:15:49:41,733 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot_llama\n",
            "goodfire (pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct), gen_kwargs: (top_p=0.9,temperature=1.0,do_sample=True), limit: 10.0, num_fewshot: 8, batch_size: auto\n",
            "|     Tasks     |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
            "|---------------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
            "|gsm8k_cot_llama|      3|flexible-extract|     8|exact_match|â†‘  |  0.2|Â±  |0.1333|\n",
            "|               |       |strict-match    |     8|exact_match|â†‘  |  0.2|Â±  |0.1333|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 5\n",
        "import os\n",
        "\n",
        "# Set environment variable for Goodfire API key if not already set\n",
        "if 'GOODFIRE_API_KEY' not in os.environ:\n",
        "    os.environ['GOODFIRE_API_KEY'] = GOODFIRE_API_KEY  # Make sure this is defined from previous cells\n",
        "\n",
        "# Run the evaluation using the command-line interface\n",
        "!python -m lm_eval \\\n",
        "    --model goodfire \\\n",
        "    --model_args pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
        "    --tasks gsm8k_cot_llama \\\n",
        "    --num_fewshot 8 \\\n",
        "    --limit 10 \\\n",
        "    --batch_size auto \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs top_p=0.9,temperature=1.0,do_sample=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2PSZgKYZfCi"
      },
      "source": [
        "#Reference: EleutherAI Eval Harness task list\n",
        "For those curious to run other evals! Please note that Min P is currently only accessible for `generate_until` tasks. There is currently no easy way to index these tasks, I just Ctrl + F'd `generate_until` on the [EleutherAI Evals Harness Repo](https://github.com/EleutherAI/lm-evaluation-harness)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xezvUGMvWPsy",
        "outputId": "ae5e9a69-2434-4e5a-98bd-8585933c1efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test response: Hello! It's nice to meet you. How\n"
          ]
        }
      ],
      "source": [
        "# Test Goodfire Client\n",
        "import goodfire\n",
        "\n",
        "client = goodfire.Client(api_key=GOODFIRE_API_KEY)\n",
        "\n",
        "# Simple test call\n",
        "response = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_completion_tokens=10\n",
        ")\n",
        "\n",
        "# Access response using ChatCompletion object attributes\n",
        "print(\"Test response:\", response.choices[0].message['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-RGcFC-PclJ"
      },
      "outputs": [],
      "source": [
        " !lm-eval --tasks list"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
